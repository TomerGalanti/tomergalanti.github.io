<!DOCTYPE HTML>
<html lang="en">

 <!-- Global Site Tag (gtag.js) - Google Analytics -->
 <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164383547-1"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-W36V9ZRFP4');
</script>

<!-- google_analytics: UA-164383547-1 -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tomer Galanti</title>

  <meta name="author" content="Tomer Galanti">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>

  <!-- TO HAVE ICONS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"> -->

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- RANDOM IMAGE! with Java Scripts -->
  <script type="text/javascript">
    var imageURLs = [
       "/images/Tomer Galanti 2.JPG"
    ];
    function getImageTag() {
      var randomIndex = Math.floor(Math.random() * imageURLs.length);
      var img = '<a href=\"'
      img += imageURLs[randomIndex];
      img += '\"><img src=\"';
      img += imageURLs[randomIndex];
      img += '\" style="width:80%;max-width:80%" alt="profile photo"></a>';
      return img;
    }
  </script>

<style>

.top-buttons {
position: absolute;
top: 20px;
right: 20px;
display: grid;
grid-template-columns: repeat(1, auto);
gap: 10px;
}

.top-buttons a {
width: 80px;
height: 30px;
display: inline-block;
background-color: white;
color: navy;
border: none;
border-radius: 5px;
cursor: pointer;
text-align: center;
text-decoration: none;
font-size: 12px;
font-family: Arial, sans-serif;
line-height: 30px;
margin-left: 10px;
}

/* .top-buttons a:hover {
background-color: #0056b3;
} */

  .content-wrapper {
      width: 50%;
      margin: 0 auto;
  }
</style>

</head>


<body>
    <!-- Navigation Buttons -->
    <div class="top-buttons">
        <a href="index.html">Home</a>
        <a href="PI.html">About the PI</a>
        <a href="publications.html">Publications</a>
        <a href="preprints.html">Preprints</a>
        <a href="activities.html">Activities</a>
        <a href="teaching.html">Teaching</a>
        <a href="group.html">Group</a>
        <a href="apply.html">Apply</a>
    </div>

    <!-- Main Content Wrapper -->
    <div class="content-wrapper">
        <!-- Name -->
        <p style="font-size: 24px;">Tomer Galanti</p>

        <!-- About Section with Image -->
        <div class="about-section">
            <!-- Text Content -->
            <div class="text-content">
                <h2>About me</h2>
                <p>
                    Howdy! üëã I am an Assistant Professor of Computer Science at <a href="https://engineering.tamu.edu/cse/index.html" class="links">Texas A&M University</a>. Previously, I was a postdoctoral associate at <a href="https://cbmm.mit.edu" class="links">MIT's Center for Brains, Minds & Machines (CBMM)</a>, collaborating with <a href="https://scholar.google.com/citations?user=WgAGy7wAAAAJ&hl=en" class="links">Tomaso Poggio</a>. I received my Ph.D. in computer science from <a href="https://en-exact-sciences.tau.ac.il/computer" class="links">Tel Aviv University</a>, where I worked with <a href="https://scholar.google.co.il/citations?user=UbFrXTsAAAAJ&hl=en" class="links">Lior Wolf</a>. In the summer of 2021, I served as a Research Scientist Intern with the Foundations team at <a href="https://www.deepmind.com/" class="links">Google DeepMind</a>, where I worked with <a href="https://scholar.google.com/citations?user=822ujacAAAAJ&hl=en" class="links">Andras Gyorgy</a> and <a href="https://scholar.google.com/citations?user=7hmCntEAAAAJ&hl=en" class="links">Marcus Hutter</a>.
                </p>
                <!-- Contact Links -->
                <div class="contact-links">
                    Email:&nbsp;&nbsp;<a href="mailto:galanti@tamu.edu">galanti at tamu dot edu</a><br>
                    Social media: &nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=ut_ISVIAAAAJ&hl=eng"><span class="ai ai-google-scholar ai"></span></a>&nbsp;
                    <a href="https://github.com/TomerGalanti"><span class="social-icon fa fa-github"></span></a>&nbsp;
                    <a href="https://www.linkedin.com/in/tomer-galanti-5880b1104/"><span class="social-icon fa fa-linkedin"></span></a>&nbsp;
                    <a href="https://x.com/GalantiTomer"><span class="social-icon fa fa-twitter"></span></a>
                </div>
            </div>

            <!-- Image Content -->
            <div class="image-content">
                <script type="text/javascript">
                    document.write(getImageTag());
                </script>
            </div>
        </div>

        <!-- Research Section -->
        <h2>Research</h2>
        <p>
            Utilizing both theory and experiments, my research develops realistic models of contemporary learning settings to guide practices in deep learning, LLMs, and AI. Some of my recent and past work explores:
        </p>
        <ul>
            <li>
                <em><strong>What types of representations are learned by neural networks?</strong></em> In [<a href="https://arxiv.org/abs/2206.05794">1</a>] we showed for the first time how the rank of the weight matrices are controlled by the training hyperparameters in modern neural networks (with residual connections, self-attention, convolutional layers, etc.). In [<a href="https://openreview.net/forum?id=162TqkUNPO">2</a>, <a href="https://proceedings.mlr.press/v202/rangamani23a.html">3</a>], we demonstrated how neural collapse propagates into intermediate layers of trained classifiers, and in [<a href="https://arxiv.org/abs/2305.15614">4</a>], we showed that self-supervised learning algorithms produce representations that cluster based on semantic attributes. Recently, in [<a href="https://arxiv.org/abs/2410.03006">5</a>], we introduced a framework unifying key deep learning phenomena, including neural collapse and the neural Ansatz, by explaining how latent representations, weights, and neuron gradients align during training.
            </li>
            <li>
               <em><strong>How various properties of latent representations affect the model's compressibility and its ability to generalize and adapt to different tasks?</strong></em> For example, in [<a href="https://openreview.net/forum?id=SwIp410B6aQ">6</a>, <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970625.pdf">7</a>, <a href="https://openreview.net/forum?id=VrK7pKwOhT_">8</a>, <a href="https://arxiv.org/abs/2212.12532">9</a>], we developed theory and algorithms linking clustering properties, such as neural collapse, to the ability of pre-trained classifiers to adapt to downstream tasks with minimal data. For compressibility, the results in [<a href="https://arxiv.org/abs/2206.05794">1</a>] show how the training hyperparameters control the network's compressibility. Furthermore, in [<a href="https://arxiv.org/abs/2301.12033">10</a>], we showed how hard-coded architectural sparsity, such as in convolutional networks, enhances generalization.
            </li>
            <li>
                I also develop <em><strong>theory and algorithms for effectively training and accelerating LLMs</strong></em>. In [<a href="https://arxiv.org/abs/2405.14105">11</a>], we introduced the Distributed Speculative Inference (DSI) algorithm, which accelerates LLM inference using multiple GPUs. DSI introduces a novel type of task parallelism called <em>Speculation Parallelism (SP)</em>, which balances computational resources and latency by overlapping target and drafter instances. In [<a href="https://arxiv.org/abs/2409.19150v1">12</a>], we demonstrated how to effectively train Autoregressive Decision Trees as coherent and grammatically correct language models. Finally, in [<a href="">13</a>], we proposed the <em>‚ÄúFair Language Model Dilemma,‚Äù</em> asserting that increasing weight decay leads to a tendency to neglect low-frequency tokens, which is detrimental to the model‚Äôs performance since low-frequency tokens constitute the majority of the vocabulary.
            </li>
        </ul>
    </div>
</body>

</html>