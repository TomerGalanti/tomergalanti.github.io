<!DOCTYPE HTML>
<html lang="en">

 <!-- Global Site Tag (gtag.js) - Google Analytics -->
 <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164383547-1"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-W36V9ZRFP4');
</script>

<!-- google_analytics: UA-164383547-1 -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tomer Galanti</title>

  <meta name="author" content="Tomer Galanti">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>

  <!-- TO HAVE ICONS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"> -->

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- RANDOM IMAGE! with Java Scripts -->
  <script type="text/javascript">
    var imageURLs = [
       "/images/Tomer Galanti 2.JPG"
    ];
    function getImageTag() {
      var randomIndex = Math.floor(Math.random() * imageURLs.length);
      var img = '<a href=\"'
      img += imageURLs[randomIndex];
      img += '\"><img src=\"';
      img += imageURLs[randomIndex];
      img += '\" style="width:100%;max-width:100%" alt="profile photo"></a>';
      return img;
    }
  </script>

  <style>
    .top-buttons {
      position: absolute;
      top: 20px;
      right: 20px;
    }

    .top-buttons a {
      padding: 10px 20px;
      background-color: #007BFF;
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      text-align: center;
      text-decoration: none;
      font-size: 16px;
      font-family: Arial, sans-serif;
      margin-left: 10px;
    }

    .top-buttons a:hover {
      background-color: #0056b3;
    }
  </style>

</head>

<body>

<div class="top-buttons">
<a href="publications.html">Publications</a>
<a href="apply.html">Apply</a>
<a href="group.html">Group</a>
</div>

<table
style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr style="padding:0px">
<td style="padding:0px">
<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr style="padding:0px">
<td style="padding:2.5%;width:63%;vertical-align:middle">
<p style="text-align:center">
<name>Tomer Galanti</name>
</p>


<br>

<p> Howdy! ðŸ‘‹ 

<br><br>
I am an Assistant Professor of Computer Science at <a href="https://engineering.tamu.edu/cse/index.html" class="links">Texas A&M University</a>. Previously, I was a postdoctoral associate at <a href="https://cbmm.mit.edu" class="links">MIT's Center for Brains, Minds & Machines (CBMM)</a>, collaborating with <a href="https://mcgovern.mit.edu/profile/tomaso-poggio/" class="links">Tomaso Poggio</a>. I recieved my Ph.D. in computer science from <a href="https://en-exact-sciences.tau.ac.il/computer" class="links">Tel Aviv University</a>, where I worked with <a href="https://www.cs.tau.ac.il/~wolf/" class="links">Lior Wolf</a>. In the summer of 2021, I served as a Research Scientist Intern with the Foundations team at <a href="https://www.deepmind.com/" class="links">Google DeepMind</a>, where I worked with <a href="https://scholar.google.com/citations?user=822ujacAAAAJ&hl=en" class="links">Andras Gyorgy</a> and <a href="https://researchers.anu.edu.au/researchers/hutter-m" class="links">Marcus Hutter</a>.
</p>
<p style="text-align:center;font-family:monospace;">
<a href="mailto:galanti@tamu.edu"><i class="fa fa-envelope"></i> &nbsp  galanti at tamu dot edu  &nbsp <i class="fa fa-envelope"></i></a>
</p>

<hr>
<div class="col-lg-4 text-center" style="text-align: center;">
<div class="profile">
<a href="https://github.com/TomerGalanti"><span  class="social-icon fa fa-github"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
<!-- <a href="https://join.skype.com/invite/kobWyHxDkzse"><span  class="social-icon fa fa-skype"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
<a href="https://scholar.google.com/citations?user=ut_ISVIAAAAJ&hl=en"><span class="ai ai-google-scholar ai"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
<!-- <a href="https://www.instagram.com/prbn96/?hl=en"><span  class="social-icon fa fa-instagram"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
<a href="https://www.linkedin.com/in/tomer-galanti-5880b1104/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
<a href="https://x.com/GalantiTomer"><span class="social-icon fa fa-twitter"></span></a>
</div>
</div>
<hr>


</td>
<td style="padding:2.5%;width:37%;max-width:37%">
<script type="text/javascript">
document.write(getImageTag());
</script>
<!-- <img style="width:100%;max-width:100%" alt="profile photo" src="images/quadro.jpg" class="hoverZoomLink"> -->
</td>
</tr>
</tbody>
</table>

<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding:20px;width:100%;vertical-align:middle"
onmouseover="document.getElementById('hidden_research').style.display = 'block';"
onmouseout="document.getElementById('hidden_research').style.display='none';">
<heading>Research</heading>
<p>
<div>
My research develops realistic models of contemporary learning settings to guide practices in deep learning, LLMs, and AI. Utilizing both theory and experiments, I study <strong>fundamental questions</strong> in the field of deep learning. Some of my recent and past work explores how to <strong>speed-up the inference of LLMs</strong> using speculative inference on multiple GPUs <a href="https://arxiv.org/abs/2405.14105">[1]</a>, <strong>what kinds of data representations</strong> we learn with self-supervised learning algorithms <a href="https://arxiv.org/abs/2305.15614">[2]</a>, <strong>why transfer learning works with very few samples</strong> <a href="https://openreview.net/forum?id=SwIp410B6aQ">[3]</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-031-19836-6_36">[4]</a>, why <strong>residual connections promote training stability</strong> in very deep networks <a href="https://proceedings.mlr.press/v161/littwin21a.html">[5]</a>, and how the training hyperparameters <strong>control the compressibility</strong> of large neural networks <a href="https://arxiv.org/abs/2206.05794">[6]</a>. See the full list of publications for more information.
</div>                  
</p>
</td>
</tr>
</tbody>
</table>

<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding:20px;width:100%;vertical-align:middle">
<heading>Activities</heading>
</td>
</tr>
</tbody>
</table>
<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td width="100%" valign="middle" style="padding-left:20px">
<details>
<summary>Summer 2024: Together with Tomaso Poggio, I organized a two-day Deep Learning Theory workshop at the CBMM Summer Course. 
Recorded talks will be uploaded soon.
<br></summary>
</details>
</td>
</tr>
<tr>
<td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
<details>
<summary>Summer 2023: Together with Tomaso Poggio, Brian Cheung and Akshay Rangamani, I organized a two-day Deep Learning Theory workshop at the CBMM Summer Course. 
Check out the talks by <a href="https://www.youtube.com/watch?v=9dXxQx-lQ2c">Surya Ganguli</a>, <a href="https://www.youtube.com/watch?v=pad023JIXVA&t=337s" class="links">Boris Hanin</a>, <a href="https://www.youtube.com/watch?v=mSX9CCKdBDA" class="links">Santosh Vempala</a>, <a href="https://www.youtube.com/watch?v=mBeb1UF9njo" class="links">Tomaso Poggio</a>, <a href="https://www.youtube.com/watch?v=K3JOQVflvyM" class="links">Dmitry Krotov</a>, <a href="https://www.youtube.com/watch?v=woS7ZyK7--Y&t=2622s" class=""links>Akshay Rangamani</a>, and <a href="https://www.youtube.com/watch?v=mSX9CCKdBDA" class="links">Myself</a>.
<br></summary>
</details>
</td>
</tr>
</tbody>
</table>

<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding:20px;width:100%;vertical-align:middle">
<heading>Publications</heading>
</td>
</tr>
<tr>
<td style="padding:20px;width:95%;vertical-align:middle;">
<div style="padding-left:0px;">
<p><a href="https://arxiv.org/abs/2301.12033"><strong>Norm-Based Generalization Bounds for Sparse Neural Networks</strong></a><br />
T. Galanti, M. Xu, L. Galanti, T. Poggio<br />
<em>Neural Information Processing Systems,&nbsp;</em>NeurIPS, 2023<em>.</em></p>
<p><a href="https://arxiv.org/abs/2305.15614"><strong>Reverse Engineering Self-Supervised Learning</strong></a><br />
I. Ben-Shaul, R. Shwartz-Ziv*, T. Galanti*, S. Dekel, Y. LeCun<br />
<em>Neural Information Processing Systems, </em>NeurIPS, 2023<em>.</em></p>
<p><a href="https://openreview.net/forum?id=162TqkUNPO"><strong>Comparative Generalization Bounds for Deep Neural Networks</strong></a><br />
T. Galanti, L. Galanti, I. Ben-Shaul<br />
<em>Transactions in Machine Learning Research</em>, TMLR, 2023.</p>

<p><a href="https://arxiv.org/abs/2301.04605"><strong>Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions</strong></a><br />
I. Ben-Shaul, T. Galanti, S. Dekel<br />
<em>Transactions in Machine Learning Research</em>, TMLR, 2023.</p>

<p><a href="https://cbmm.mit.edu/sites/default/files/publications/Feature_Learning_memo.pdf"><strong>Feature Learning in Deep Classifiers Through Intermediate Neural Collapse</strong></a><br />
A. Rangamani, M. Lindegaard, T. Galanti, T. Poggio<br />
<em>International Conference on Machine Learning</em>,<em> </em>ICML,<em>&nbsp;</em>2023<em>.</em></p>

<p><a href="https://spj.science.org/doi/10.34133/research.0024"><strong>Dynamics of Deep Classifiers Trained with the Square Loss: Normalization, Low Rank, Neural Collapse and Generalization Bounds</strong></a><br />
M. Xu, A. Rangamani, Q. Liao, T. Galanti, T. Poggio<br />
<em>Research (a Science partner journal),&nbsp;</em>2023<em>.</em></p>

<p><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970625.pdf"><strong>Image2Point: 3D Point-Cloud Understanding with 2D Image Pretrained Models</strong></a><br />
C. Xu, S. Yang, T. Galanti, B. Wu, X. Yue, B. Zhai, W. Zhan, P. Vajda, K. Keutzer, M. Tomizuka<br />
<em>IEEE European Conference on Computer Vision</em>, ECCV, 2022.</p>

<p><a href="https://openreview.net/pdf?id=VrK7pKwOhT_"><strong>Improved Generalization Bounds for Transfer Learning via Neural Collapse</strong></a><br />
T. Galanti, A. Gyorgy, M. Hutter<br />
<em>ICML Workshop on Pretraining: Perspectives, Pitfalls and Paths Forward, </em>2022<em>.</em></p>

<p><a href="https://arxiv.org/abs/2202.09028"><strong>On the Implicit Bias Towards Depth Minimization in Deep Neural Networks</strong></a><br />
T. Galanti, L. Galanti, I. Ben-Shaul<br />
<em>Conference on the Mathematical Theory of Deep Neural Networks</em>, DEEPMATH, 2022.<br />
<em>Workshop on the Theory of Overparameterized Machine Learning</em>, TOPML, 2022.</p>

<p><a href="https://openreview.net/forum?id=SwIp410B6aQ"><strong>On the Role of Neural Collapse in Transfer Learning</strong></a><br />
T. Galanti, A. Gyorgy, M. Hutter<br />
<em>International Conference on Learning Representations</em>, ICLR, 2022.</p>

<p><a href="https://proceedings.mlr.press/v177/ali22a.html"><strong>Weakly Supervised Discovery of Semantic Attributes</strong></a><br />
A. A. Ali, T. Galanti, E. Zheltonozhskii, C. Baskin, L. Wolf<br />
<em>Causal Learning and Reasoning</em>, CLeaR, 2022.</p>

<p><a href="https://openreview.net/forum?id=xfskdMFkuTS"><strong>Meta Internal Learning</strong></a><br />
R. Ben Sadoun, S. Gur, T. Galanti, L. Wolf<br />
<em>Neural Information Processing Systems,&nbsp;</em>NeurIPS, 2021.</p>

<p><a href="https://arxiv.org/abs/2001.10460"><strong>On Random Kernels of Residual Architectures</strong></a><br />
E. Littwin*, T. Galanti*, L. Wolf<br />
<em>Uncertainty in Artificial Intelligence</em>, UAI, 2021.</p>

<p><a href="https://jmlr.org/papers/v22/18-489.html"><strong>Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs</strong></a><br />
T. Galanti, S. Benaim, L. Wolf<br />
<em>Journal of Machine Learning Research</em>, JMLR, 2021.</p>

<p><a href="https://link.springer.com/article/10.1007/s11263-020-01424-w"><strong>Evaluation Metrics for Conditional Image Generation</strong></a><br />
Y. Benny, T. Galanti, S. Benaim, L. Wolf<br />
<em>International Journal of Computer Vision</em>, IJCV, 2021.</p>

<p><a href="https://proceedings.neurips.cc/paper/2020/hash/75c58d36157505a600e0695ed0b3a22d-Abstract.html"><strong>On the Modularity of Hypernetworks&nbsp;</strong></a><br />
T. Galanti, L. Wolf<br />
<em>Neural Information Processing Systems</em>, NeurIPS, 2020 <strong>(oral presentation - 1% acceptance)</strong>.</p>

<p><a href="https://proceedings.neurips.cc/paper/2020/hash/999df4ce78b966de17aee1dc87111044-Abstract.html"><strong>On Infinite-Width Hypernetworks</strong></a><br />
E. Littwin*, T. Galanti*, L. Wolf<br />
<em>Neural Information Processing Systems</em>, NeurIPS, 2020.</p>

<p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Benaim_Domain_Intersection_and_Domain_Difference_ICCV_2019_paper.html"><strong>Domain Intersection and Domain Difference</strong></a><br />
S. Benaim, M. Khaitov, T. Galanti, L. Wolf<br />
<em>IEEE International Conference on Computer Vision</em>, ICCV, 2019.</p>

<p><strong><a href="https://openreview.net/forum?id=H1lqZhRcFm">Unsuperivsed Learning of the Set of Local Maxima</a> </strong><br />
L. Wolf, S. Benaim, T. Galanti<br />
<em>International Conference on Learning Representations</em>, ICLR, 2019.</p>

<p><a href="https://openreview.net/forum?id=BylE1205Fm"><strong>Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer</strong></a><br />
O. Press, T. Galanti, S. Benaim, L. Wolf<br />
<em>International Conference on Learning Representations</em>, ICLR, 2019.</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3306618.3314260"><strong>A Formal Approach to Explainability</strong></a><br />
L. Wolf, T. Galanti, T. Hazan<br />
<em>Artificial Intelligence, Ethics and Society</em>, AIES, 2019.</p>

<p><a href="http://nips2018dltheory.rice.edu/"><strong>Generalization Bounds for Cross-Domain Mapping with WGANs</strong></a><br />
T. Galanti, S. Benaim, L. Wolf<br />
<em>NeurIPS Workshop on Integration of Deep Learning Theories</em>, 2018.</p>

<p><a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Lior_Wolf_Estimating_the_Success_ECCV_2018_paper.html"><strong>Estimating the Success of Unsupervised Image to Image Translation</strong></a><br />
S. Benaim*, T. Galanti*, L. Wolf<br />
<em>IEEE European Conference on Computer Vision</em>, ECCV, 2018.</p>

<p><a href="https://openreview.net/forum?id=H1VjBebR-"><strong>The Role of Minimal Complexity in Unsupervised Learning of Semantic Mappings</strong></a><br />
T. Galanti, L. Wolf, S. Benaim<br />
<em>International Conference on Learning Representations</em>, ICLR, 2018.</p>

<p><a href="https://academic.oup.com/imaiai/article-abstract/5/2/159/2363463?redirectedFrom=fulltext"><strong>A Theoretical Framework for Deep Transfer Learning</strong></a><br />
T. Galanti, T. Hazan, L. Wolf<br />
<em>Information and Inference: A Journal of the IMA</em>, IMAIAI, 2016.<br />        
</div>
</td>
</tr>
</tbody>
</table>

<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding:20px;width:100%;vertical-align:middle">
<heading>Teaching</heading>
</td>
</tr>
</tbody>
</table>
<table
style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<tr>
<td style="padding-left:25px;width:5%;vertical-align:middle">
<i class="fa fa-graduation-cap" aria-hidden="true"></i>
</td>
<td width="95%" valign="middle" style="padding-left:20px">
<details>
<summary><a href="" class="links">Special Topics in Recent Developments in Deep Learning and Large Language Models</a> 
, Texas A&M University, Fall 2024.
<br></summary>
</details>
</td>
</tr>

<tr>
<td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
<i class="fa fa-graduation-cap" aria-hidden="true"></i>
</td>
<td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
<details>
<summary><a href="https://poggio-lab.mit.edu/F23-9-520" class="links">Statistical Learning Theory and its Applications</a> 
, Massachusetts Institute of Technology, Fall 2023.
<br></summary>
</details>
</td>
</tr>

<tr>
<td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
<i class="fa fa-graduation-cap" aria-hidden="true"></i>
</td>
<td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
<details>
<summary><a href="https://poggio-lab.mit.edu/F22-9-520" class="links">Statistical Learning Theory and its Applications</a>
, Massachusetts Institute of Technology, Fall 2022.
<br></summary>
</details>
</td>
</tr>

<tr>
<td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
<i class="fa fa-graduation-cap" aria-hidden="true"></i>
</td>
<td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
<details>
<summary>Deep Convolutional Neural Networks, Tel Aviv University, Spring 2020.
<br></summary>
</details>
</td>
</tr>

<tr>
<td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
<i class="fa fa-graduation-cap" aria-hidden="true"></i>
</td>
<td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
<details>
<summary>Deep Convolutional Neural Networks, Tel Aviv University, Spring 2019.
<br></summary>
</details>
</td>
</tr>

</tbody>
</table>

<br><br>
</td>
</tr>
</table>
</body>

</html>