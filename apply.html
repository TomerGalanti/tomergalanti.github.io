<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Research</title>
  </head>
</html>


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <title>Research</title>
  
    <meta name="author" content="Tomer Galanti">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  
    <!-- Academicons -->
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>
  
    <!-- TO HAVE ICONS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"> -->
  
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  
    <!-- Icon for the browser tab -->
    <link rel="icon" type="image/png" href="/images/princeton.png"/>
  
  </head>
  

<heading>Past research</heading>

## Neural network approximation theory

The goal is to understand which is the family of functions which are approximable without curse of dimensionality.

Coauthors: <a href="https://scholar.google.de/citations?user=fymm-XQAAAAJ&hl=en" class="links">Prof. Arnulf Jentzen</a>, <a href="https://people.math.ethz.ch/~patrickc/" class="links">Prof. Patrick Cheridito</a>, <a href="https://scholar.google.com/citations?user=Dc8yKjUAAAAJ&hl=en" class="links">Philippe von Wurstemberger</a>, Robin Graeber.

<details style = "font-size:0.7em;">
  <summary style = "font-size:1.2em;"><a href="https://arxiv.org/abs/2012.04326" class="links">High-dimensional approximation spaces of artificial neural networks and applications to partial differential equations.</a></summary>

  This work has been my "semester paper" during my degree at ETH.

  In this work we develop a new machinery to study the capacity of neural networks to approximate high-dimensional functions without suffering from the curse of dimensionality. We then use our machinery to prove that the solutions of certain easy PDEs are arbitrarily approximable without the curse of dimensionality.

</details>

<details style = "font-size:0.7em;">
  <summary style = "font-size:1.2em;">Deep neural network approximations for high-dimensional functions. (to appear soon)</summary>

  Soon on arXiv, this work has been my thesis during my degree at ETH.

  On the line of the previous work, but more general, we provide a suitable large class of functions that can be approximated by DNNs without the curse of dimensionality. The main contributions of this thesis are the following: (a) the discovery of new cost bounds in the approximation of the product of d âˆˆ N real numbers and of representation the maximum of d real numbers, (b) the introduction of some DNN approximation spaces of functions and the proof that they are closed for some operations, and, as a consequence, (c) the proof that DNNs overcome the curse of dimensionality in the approximation on any compact set of products, maxima or the combination of both applied to low dimensional locally Lipschitz continuous functions.

</details>





<hr>

<div class="col-lg-4 text-center">
  <div class="profile">
      <!-- <img src="./images/face_016.jpg" width="120" height="160" > -->
      <!-- <h2>Contacts</h2> -->
      <!-- <img height="20" width="20" src="./assets/icons/mail.svg" /> <a class="contact-link"href=""> galanti@tamu.edu</a>
      <img height="20" width="20" src="./assets/icons/graduation.svg" /> <a class="contact-link"href="https://scholar.google.com/citations?hl=en&pli=1&user=ut_ISVIAAAAJ"> Google Scholar</a>
      <a href="https://www.linkedin.com/in/tomer-galanti-5880b1104/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp;  &nbsp; 
      <a href="https://github.com/TomerGalanti"><span  class="social-icon fa fa-github"></span></a> &nbsp;  &nbsp; 
      <a href="https://scholar.google.com/citations?hl=en&pli=1&user=ut_ISVIAAAAJ"><span class="ai ai-fw ai-google-scholar-square"></span></a> &nbsp;  &nbsp; 
      <a href="mailto:galanti@tamu.edu"><span class="social-icon fa fa-envelope"></span></a>
  </div>
</div>

<hr>






## Responsible mathematical modeling
We are writing a short comment on the on the limits of mathematical modeling, statistics, and machine learning. The focus is on the O'neil conjecture and the fact that it happens that modeler do not account for it, in a sort of <em>modeler hubris</em>.

Coauthors: <a href="https://scholar.google.com/citations?user=vqhLsGkAAAAJ&hl=en" class="links">Andrea Saltelli</a>, <a href="https://scholar.google.com/citations?user=3h35F_4AAAAJ&hl=en" class="links">Tommaso Portaluri</a>, <a href="https://scholar.google.com/citations?user=lGgh0DoAAAAJ&hl=en" class="links">Arnald Puy</a>, and <a href="https://scholar.google.com/citations?user=NyVVh7kAAAAJ&hl=en" class="links">Samuele Lo Piano</a>.



Related:
<details style = "font-size:0.7em;">
  <summary style = "font-size:1.2em;">Organizing the conference: <a href="https://inetxoecd.associazionecest.it/" class="links">Forecasting the future for sustainable development: New Approaches to Modelling and the Science of Prediction</a></summary>

  With a group of friends at <a href="https://www.associazionecest.it/en" class="links">CEST</a> 
  I'm organizing a conference on "<i>Forecasting the future for sustainable development: New
  Approaches to Modelling and the Science of Prediction</i>" supported by <a href="https://ysi.ineteconomics.org/" class="links">INET</a>
  and hosted by <a href="https://www.oecd.org/" class="links">OECD</a>. 
  Please <a href="https://inetxoecd.associazionecest.it/" class="links">check out the website</a>! 
  <br>
  I will be chair for the session "<i>XAI and sustainable data science</i>" and I'm glad to announce that  
  <a href="https://scholar.google.com/citations?user=mezKJyoAAAAJ&hl=en" class="links">Prof. Cynthia Rudin</a> will give us a lecture.
  We will have also many others amazing guests and interesting sessions, please check out!


</details>



<hr>

<div class="col-lg-4 text-center">
  <div class="profile">
      <!-- <img src="./images/face_016.jpg" width="120" height="160" > -->
      <!-- <h2>Contacts</h2> -->
      <!-- <img height="20" width="20" src="./assets/icons/mail.svg" /> <a class="contact-link"href=""> pierb@princeton.edu</a>
      <img height="20" width="20" src="./assets/icons/graduation.svg" /> <a class="contact-link"href="https://scholar.google.com/citations?user=spL439oAAAAJ&hl=en"> Google Scholar</a>
      <img height="20" width="20" src="./assets/icons/user.svg" /> <a class="contact-link"href="https://pierbeneventano.github.io/CV/CV_Beneventano.pdf"> Curriculum vitae</a> -->
      <a href="https://www.linkedin.com/in/pierbeneventano/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp;  &nbsp;  
      <a href="https://join.skype.com/invite/kobWyHxDkzse"><span  class="social-icon fa fa-skype"></span></a> &nbsp;  &nbsp; 
      <a href="https://www.instagram.com/prbn96/?hl=en"><span  class="social-icon fa fa-instagram"></span></a> &nbsp;  &nbsp; 
      <a href="https://github.com/PierBeneventano"><span  class="social-icon fa fa-github"></span></a> &nbsp;  &nbsp; 
      <a href="https://pierbeneventano.github.io/CV/CV_Beneventano.pdf" class="links"> CV </a> &nbsp;  &nbsp; 
      <a href="https://scholar.google.com/citations?user=spL439oAAAAJ&hl=en"><span class="ai ai-fw ai-google-scholar-square"></span></a> &nbsp;  &nbsp; 
      <a href="mailto:pierb@princeton.edu"><span class="social-icon fa fa-envelope"></span></a>
  </div>
</div>

<hr>