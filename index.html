<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Tomer Galanti</title>
    <meta name="author" content="Tomer Galanti">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href="https://fonts.googleapis.com/css2?family=Amiri:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164383547-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-W36V9ZRFP4');
    </script>
    <script type="text/javascript">
        var imageURLs = ["/images/Tomer Galanti 2.JPG"];
        function getImageTag() {
            var randomIndex = Math.floor(Math.random() * imageURLs.length);
            return `<a href="${imageURLs[randomIndex]}"><img src="${imageURLs[randomIndex]}" style="width:30%; max-width:30%" alt="profile photo"></a>`;
        }
    </script>
    <style>
        .content-wrapper {
            width: 50%;
            margin: 0 auto;
        }
        .top-buttons {
            position: absolute;
            top: 20px;
            right: 20px;
            display: grid;
            grid-template-columns: repeat(1, auto);
            gap: 10px;
        }
        .top-buttons a {
            width: 80px;
            height: 30px;
            display: inline-block;
            background-color: white;
            color: navy;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-align: center;
            text-decoration: none;
            font-size: 12px;
            font-family: Arial, sans-serif;
            line-height: 30px;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="top-buttons">
        <a href="index.html">Home</a>
        <a href="publications.html">Publications</a>
        <a href="preprints.html">Preprints</a>
        <a href="activities.html">Activities</a>
        <a href="teaching.html">Teaching</a>
        <a href="group.html">Group</a>
        <a href="apply.html">Apply</a>
    </div>
    <div class="content-wrapper">
        <div style="text-align:center;">
            <p style="font-size: 24px;">Tomer Galanti</p>
            <div>
                <script type="text/javascript">
                    document.write(getImageTag());
                </script>
            </div>
        </div>
        <!-- <p style="font-family: 'Caveat', cursive; font-size: 18px; text-align:center;">Howdy! üëã</p> -->
        <!-- <p style="text-align:center;">
            
        </p> -->


        <div style="max-width: 400px; margin: auto;">
        <hr>
        <div class="col-lg-4 text-center" style="text-align: center;">
        <div class="profile">
        <a href="mailto:galanti@tamu.edu"><i class="fa fa-envelope"></i> &nbsp;galanti at tamu dot edu &nbsp;<i class="fa fa-envelope"></i></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp
        <a href="https://github.com/TomerGalanti"><span  class="social-icon fa fa-github"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
        <a href="https://scholar.google.com/citations?user=ut_ISVIAAAAJ&hl=en"><span class="ai ai-google-scholar ai"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
        <a href="https://www.linkedin.com/in/tomer-galanti-5880b1104/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
        <a href="https://x.com/GalantiTomer"><span class="social-icon fa fa-twitter"></span></a>
        </div>
        </div>
        <hr>
        </div>
      <br><br>
        <div>
            <heading>About me</heading>
            <p>
              Howdy! üëã I am an Assistant Professor of Computer Science at <a href="https://engineering.tamu.edu/cse/index.html" class="links">Texas A&M University</a>. Previously, I was a postdoctoral associate at <a href="https://cbmm.mit.edu" class="links">MIT's Center for Brains, Minds & Machines (CBMM)</a>, collaborating with <a href="https://mcgovern.mit.edu/profile/tomaso-poggio/" class="links">Tomaso Poggio</a>. I recieved my Ph.D. in computer science from <a href="https://en-exact-sciences.tau.ac.il/computer" class="links">Tel Aviv University</a>, where I worked with <a href="https://www.cs.tau.ac.il/~wolf/" class="links">Lior Wolf</a>. In the summer of 2021, I served as a Research Scientist Intern with the Foundations team at <a href="https://www.deepmind.com/" class="links">Google DeepMind</a>, where I worked with <a href="https://scholar.google.com/citations?user=822ujacAAAAJ&hl=en" class="links">Andras Gyorgy</a> and <a href="https://researchers.anu.edu.au/researchers/hutter-m" class="links">Marcus Hutter</a>.
            </p>
            <heading>Research</heading>
            <p>
            Utilizing both theory and experiments, my research develops realistic models of contemporary learning settings to guide practices in deep learning, LLMs, and AI. Some of my recent and past work explores:
            <ul>
              <li style="margin-bottom: 10px;">My work explores the <em><strong>types of representations learned by neural networks</strong></em>. In [<a href="https://arxiv.org/abs/2206.05794">1</a>], we were the first to show how the training hyperparameters influence the rank of weight matrices in modern neural networks (with residual connections, self-attention, convolutional layers, etc.). In [<a href="https://openreview.net/forum?id=162TqkUNPO">2</a>, <a href="https://proceedings.mlr.press/v202/rangamani23a.html">3</a>], we demonstrated how neural collapse propagates into intermediate layers of trained classifiers, and in [<a href="https://arxiv.org/abs/2305.15614">4</a>], we showed that self-supervised learning algorithms produce representations that cluster based on semantic attributes. Recently, in [<a href="https://arxiv.org/abs/2410.03006">5</a>], we introduced a framework unifying key deep learning phenomena, including neural collapse and the neural Ansatz, by explaining how latent representations, weights, and neuron gradients align during training.</li>
              <li style="margin-bottom: 10px;">Additionally, my work explores <em><strong>how various properties of neural network representations affect generalization, task adaptation, and compressibility</strong></em>. For example, in [<a href="https://openreview.net/forum?id=SwIp410B6aQ">6</a>, <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970625.pdf">7</a>, <a href="https://openreview.net/forum?id=VrK7pKwOhT_">8</a>, <a href="https://arxiv.org/abs/2212.12532">9</a>], we developed theory and algorithms linking clustering properties, such as neural collapse, to the ability of pre-trained classifiers to adapt to downstream tasks with minimal data. For compressibility, the results in [<a href="https://arxiv.org/abs/2206.05794">1</a>] show how the training hyperparameters control the network's compressibility. Furthermore, in [<a href="https://arxiv.org/abs/2301.12033">10</a>], we showed how hard-coded architectural sparsity, such as in convolutional networks, enhances generalization.</li>
              <li style="margin-bottom: 40px;">I also develop <em><strong>theory and algorithms for effectively training and accelerating LLMs</strong></em>. In [<a href="https://arxiv.org/abs/2405.14105">11</a>], we introduced the Distributed Speculative Inference (DSI) algorithm, which accelerates LLM inference using multiple GPUs. DSI introduces a novel type of task parallelism called <em>Speculation Parallelism (SP)</em>, which balances computational resources and latency by overlapping target and drafter instances. In [<a href="https://arxiv.org/abs/2409.19150v1">12</a>], we demonstrated how to effectively train Autoregressive Decision Trees as coherent and grammatically correct language models. Finally, in [<a href="">13</a>], we proposed the <em>‚ÄúFair Language Model Dilemma,‚Äù</em> asserting that increasing weight decay leads to a tendency to neglect low-frequency tokens, which is detrimental to the model‚Äôs performance since low-frequency tokens constitute the majority of the vocabulary.</li>
            </ul>
            </p>
        </div>
    </div>
</body>
</html>
