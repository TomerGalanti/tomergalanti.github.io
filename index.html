<!DOCTYPE HTML>
<html lang="en">

 <!-- Global Site Tag (gtag.js) - Google Analytics -->
 <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164383547-1"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-W36V9ZRFP4');
</script>

<!-- google_analytics: UA-164383547-1 -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tomer Galanti</title>

  <meta name="author" content="Tomer Galanti">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Academicons -->
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>

  <!-- TO HAVE ICONS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"> -->

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- RANDOM IMAGE! with Java Scripts -->
  <script type="text/javascript">
    var imageURLs = [
       "/images/Tomer Galanti 2.JPG"
    ];
    function getImageTag() {
      var randomIndex = Math.floor(Math.random() * imageURLs.length);
      var img = '<a href=\"'
      img += imageURLs[randomIndex];
      img += '\"><img src=\"';
      img += imageURLs[randomIndex];
      img += '\" style="width:100%;max-width:100%" alt="profile photo"></a>';
      return img;
    }
  </script>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Tomer Galanti</name>
                  </p>
                  

                  <br>
                  
                  <p> Howdy! ðŸ‘‹ 
                  
                    <br><br>
                    I am an Assistant Professor of Computer Science at <a href="https://engineering.tamu.edu/cse/index.html" class="links">Texas A&M University</a>. I recently completed my postdoc at <a href="https://cbmm.mit.edu" class="links">MIT's CBMM</a>, where I worked with <a href="https://mcgovern.mit.edu/profile/tomaso-poggio/" class="links">Tomaso Poggio</a>. I received my Ph.D. from <a href="https://en-exact-sciences.tau.ac.il/computer" class="links">Tel Aviv University</a>, where I worked with <a href="https://www.cs.tau.ac.il/~wolf/" class="links">Lior Wolf</a>. In the summer of 2021, I worked as a Research Scientist Intern with the Foundations team at <a href="https://www.deepmind.com/" class="links">Google DeepMind</a>, where I primarily collaborated with <a href="https://scholar.google.com/citations?user=822ujacAAAAJ&hl=en" class="links">Andras Gyorgy</a> and <a href="https://researchers.anu.edu.au/researchers/hutter-m" class="links">Marcus Hutter</a>. 
                  </p>

                  <p style="text-align:center;font-family:monospace;">
                    <a href="mailto:galanti@tamu.edu"><i class="fa fa-envelope"></i> &nbsp  galanti at tamu dot edu  &nbsp <i class="fa fa-envelope"></i></a>
                  </p>

                  <hr>
                  <div class="col-lg-4 text-center" style="text-align: center;">
                    <div class="profile">
                      <a href="https://github.com/TomerGalanti"><span  class="social-icon fa fa-github"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
                      <!-- <a href="https://join.skype.com/invite/kobWyHxDkzse"><span  class="social-icon fa fa-skype"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
                      <a href="https://scholar.google.com/citations?user=ut_ISVIAAAAJ&hl=en"><span class="ai ai-google-scholar ai"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
                      <!-- <a href="https://www.instagram.com/prbn96/?hl=en"><span  class="social-icon fa fa-instagram"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  -->
                      <a href="https://www.linkedin.com/in/tomer-galanti-5880b1104/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  &nbsp  
                      <a href="https://x.com/GalantiTomer"><span class="social-icon fa fa-twitter"></span></a>
                    </div>
                  </div>
                  <hr>


                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <script type="text/javascript">
                    document.write(getImageTag());
                  </script>
                  <!-- <img style="width:100%;max-width:100%" alt="profile photo" src="images/quadro.jpg" class="hoverZoomLink"> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle"
                  onmouseover="document.getElementById('hidden_research').style.display = 'block';"
                  onmouseout="document.getElementById('hidden_research').style.display='none';">
                  <heading>Research</heading>
                  <p>
                    <div>
                      My research develops realistic models of contemporary learning settings to guide practices in deep learning, LLMs, and AI. Utilizing both theory and experiments, I study <strong>fundamental questions</strong> in the field of deep learning. Some of my recent and past work explores how to <strong>speed-up the inference of LLMs</strong> using speculative inference on multiple GPUs <a href="https://arxiv.org/abs/2405.14105">[1]</a>, <strong>what kinds of data representations</strong> we learn with self-supervised learning algorithms <a href="https://arxiv.org/abs/2305.15614">[2]</a>, <strong>why transfer learning works with very few samples</strong> <a href="https://openreview.net/forum?id=SwIp410B6aQ">[3]</a>, <a href="https://link.springer.com/chapter/10.1007/978-3-031-19836-6_36">[4]</a>, why <strong>residual connections promote training stability</strong> in very deep networks <a href="https://proceedings.mlr.press/v161/littwin21a.html">[5]</a>, and how the training hyperparameters <strong>control the compressibility</strong> of large neural networks <a href="https://arxiv.org/abs/2206.05794">[6]</a>. See the full list of publications for more information.
                  </div>                  
                  </p>
                </td>
              </tr>
            </tbody>
          </table>




          <table style="width:100%;border:0;margin:auto;">
            <tbody>
              <tr onmouseover="document.getElementById('hidden_publications').style.display = 'block';" onmouseout="document.getElementById('hidden_publications').style.display='none';">
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <heading>Publications</heading>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;">
                  <div style="padding-left:0px;">
                    <p><a href="https://arxiv.org/abs/2301.12033"><strong>Norm-Based Generalization Bounds for Sparse Neural Networks</strong></a><br />
                      T. Galanti, M. Xu, L. Galanti, T. Poggio<br />
                      <em>Neural Information Processing Systems,&nbsp;</em>NeurIPS, 2023<em>.</em></p>
                    <p><a href="https://arxiv.org/abs/2305.15614"><strong>Reverse Engineering Self-Supervised Learning</strong></a><br />
                      I. Ben-Shaul, R. Shwartz-Ziv*, T. Galanti*, S. Dekel, Y. LeCun<br />
                      <em>Neural Information Processing Systems, </em>NeurIPS, 2023<em>.</em></p>
                      <p><a href="https://openreview.net/forum?id=162TqkUNPO"><strong>Comparative Generalization Bounds for Deep Neural Networks</strong></a><br />
                        T. Galanti, L. Galanti, I. Ben-Shaul<br />
                        <em>Transactions in Machine Learning Research</em>, TMLR, 2023.</p>
                        
                        <p><a href="https://arxiv.org/abs/2301.04605"><strong>Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions</strong></a><br />
                        I. Ben-Shaul, T. Galanti, S. Dekel<br />
                        <em>Transactions in Machine Learning Research</em>, TMLR, 2023.</p>
                        
                        <p><a href="https://cbmm.mit.edu/sites/default/files/publications/Feature_Learning_memo.pdf"><strong>Feature Learning in Deep Classifiers Through Intermediate Neural Collapse</strong></a><br />
                        A. Rangamani, M. Lindegaard, T. Galanti, T. Poggio<br />
                        <em>International Conference on Machine Learning</em>,<em> </em>ICML,<em>&nbsp;</em>2023<em>.</em></p>
                        
                        <p><a href="https://spj.science.org/doi/10.34133/research.0024"><strong>Dynamics of Deep Classifiers Trained with the Square Loss: Normalization, Low Rank, Neural Collapse and Generalization Bounds</strong></a><br />
                        M. Xu, A. Rangamani, Q. Liao, T. Galanti, T. Poggio<br />
                        <em>Research (a Science partner journal),&nbsp;</em>2023<em>.</em></p>
                        
                        <p><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136970625.pdf"><strong>Image2Point: 3D Point-Cloud Understanding with 2D Image Pretrained Models</strong></a><br />
                        C. Xu, S. Yang, T. Galanti, B. Wu, X. Yue, B. Zhai, W. Zhan, P. Vajda, K. Keutzer, M. Tomizuka<br />
                        <em>IEEE European Conference on Computer Vision</em>, ECCV, 2022.</p>
                        
                        <p><a href="https://openreview.net/pdf?id=VrK7pKwOhT_"><strong>Improved Generalization Bounds for Transfer Learning via Neural Collapse</strong></a><br />
                        T. Galanti, A. Gyorgy, M. Hutter<br />
                        <em>ICML Workshop on Pretraining: Perspectives, Pitfalls and Paths Forward, </em>2022<em>.</em></p>
                        
                        <p><a href="https://arxiv.org/abs/2202.09028"><strong>On the Implicit Bias Towards Depth Minimization in Deep Neural Networks</strong></a><br />
                        T. Galanti, L. Galanti, I. Ben-Shaul<br />
                        <em>Conference on the Mathematical Theory of Deep Neural Networks</em>, DEEPMATH, 2022.<br />
                        <em>Workshop on the Theory of Overparameterized Machine Learning</em>, TOPML, 2022.</p>
                        
                        <p><a href="https://openreview.net/forum?id=SwIp410B6aQ"><strong>On the Role of Neural Collapse in Transfer Learning</strong></a><br />
                        T. Galanti, A. Gyorgy, M. Hutter<br />
                        <em>International Conference on Learning Representations</em>, ICLR, 2022.</p>
                        
                        <p><a href="https://proceedings.mlr.press/v177/ali22a.html"><strong>Weakly Supervised Discovery of Semantic Attributes</strong></a><br />
                        A. A. Ali, T. Galanti, E. Zheltonozhskii, C. Baskin, L. Wolf<br />
                        <em>Causal Learning and Reasoning</em>, CLeaR, 2022.</p>
                        
                        <p><a href="https://openreview.net/forum?id=xfskdMFkuTS"><strong>Meta Internal Learning</strong></a><br />
                        R. Ben Sadoun, S. Gur, T. Galanti, L. Wolf<br />
                        <em>Neural Information Processing Systems,&nbsp;</em>NeurIPS, 2021.</p>
                        
                        <p><a href="https://arxiv.org/abs/2001.10460"><strong>On Random Kernels of Residual Architectures</strong></a><br />
                        E. Littwin*, T. Galanti*, L. Wolf<br />
                        <em>Uncertainty in Artificial Intelligence</em>, UAI, 2021.</p>
                        
                        <p><a href="https://jmlr.org/papers/v22/18-489.html"><strong>Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs</strong></a><br />
                        T. Galanti, S. Benaim, L. Wolf<br />
                        <em>Journal of Machine Learning Research</em>, JMLR, 2021.</p>
                        
                        <p><a href="https://link.springer.com/article/10.1007/s11263-020-01424-w"><strong>Evaluation Metrics for Conditional Image Generation</strong></a><br />
                        Y. Benny, T. Galanti, S. Benaim, L. Wolf<br />
                        <em>International Journal of Computer Vision</em>, IJCV, 2021.</p>
                        
                        <p><a href="https://proceedings.neurips.cc/paper/2020/hash/75c58d36157505a600e0695ed0b3a22d-Abstract.html"><strong>On the Modularity of Hypernetworks&nbsp;</strong></a><br />
                        T. Galanti, L. Wolf<br />
                        <em>Neural Information Processing Systems</em>, NeurIPS, 2020 <strong>(oral presentation - 1% acceptance)</strong>.</p>
                        
                        <p><a href="https://proceedings.neurips.cc/paper/2020/hash/999df4ce78b966de17aee1dc87111044-Abstract.html"><strong>On Infinite-Width Hypernetworks</strong></a><br />
                        E. Littwin*, T. Galanti*, L. Wolf<br />
                        <em>Neural Information Processing Systems</em>, NeurIPS, 2020.</p>
                        
                        <p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Benaim_Domain_Intersection_and_Domain_Difference_ICCV_2019_paper.html"><strong>Domain Intersection and Domain Difference</strong></a><br />
                        S. Benaim, M. Khaitov, T. Galanti, L. Wolf<br />
                        <em>IEEE International Conference on Computer Vision</em>, ICCV, 2019.</p>
                        
                        <p><strong><a href="https://openreview.net/forum?id=H1lqZhRcFm">Unsuperivsed Learning of the Set of Local Maxima</a> </strong><br />
                        L. Wolf, S. Benaim, T. Galanti<br />
                        <em>International Conference on Learning Representations</em>, ICLR, 2019.</p>
                        
                        <p><a href="https://openreview.net/forum?id=BylE1205Fm"><strong>Emerging Disentanglement in Auto-Encoder Based Unsupervised Image Content Transfer</strong></a><br />
                        O. Press, T. Galanti, S. Benaim, L. Wolf<br />
                        <em>International Conference on Learning Representations</em>, ICLR, 2019.</p>
                        
                        <p><a href="https://dl.acm.org/doi/abs/10.1145/3306618.3314260"><strong>A Formal Approach to Explainability</strong></a><br />
                        L. Wolf, T. Galanti, T. Hazan<br />
                        <em>Artificial Intelligence, Ethics and Society</em>, AIES, 2019.</p>
                        
                        <p><a href="http://nips2018dltheory.rice.edu/"><strong>Generalization Bounds for Cross-Domain Mapping with WGANs</strong></a><br />
                        T. Galanti, S. Benaim, L. Wolf<br />
                        <em>NeurIPS Workshop on Integration of Deep Learning Theories</em>, 2018.</p>
                        
                        <p><a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Lior_Wolf_Estimating_the_Success_ECCV_2018_paper.html"><strong>Estimating the Success of Unsupervised Image to Image Translation</strong></a><br />
                        S. Benaim*, T. Galanti*, L. Wolf<br />
                        <em>IEEE European Conference on Computer Vision</em>, ECCV, 2018.</p>
                        
                        <p><a href="https://openreview.net/forum?id=H1VjBebR-"><strong>The Role of Minimal Complexity in Unsupervised Learning of Semantic Mappings</strong></a><br />
                        T. Galanti, L. Wolf, S. Benaim<br />
                        <em>International Conference on Learning Representations</em>, ICLR, 2018.</p>
                        
                        <p><a href="https://academic.oup.com/imaiai/article-abstract/5/2/159/2363463?redirectedFrom=fulltext"><strong>A Theoretical Framework for Deep Transfer Learning</strong></a><br />
                        T. Galanti, T. Hazan, L. Wolf<br />
                        <em>Information and Inference: A Journal of the IMA</em>, IMAIAI, 2016.<br />        
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Activities</heading>
                  <p>
                    <div>
                      <tr>
                        <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                          <details>
                            <summary>  
                              Together with Tomaso Poggio, I organized a two-day Deep Learning Theory workshop at the CBMM Summer Course.
                              <br></summary>
                          </details>
                        </td>
                      </tr>

                      <tr>
                        <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                          <details>
                            <summary><a href="https://registrar.princeton.edu/course-offerings/course-details?term=1242&courseid=012833" class="links">Computing and Optimization for the Physical and Social Sciences</a> 
                              , Princeton University, Fall 2023.
                              <br></summary>
                          </details>
                        </td>
                      </tr>

                      <tr>
                        <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                          <details>
                            <summary><a href="https://registrar.princeton.edu/course-offerings/course-details?term=1242&courseid=012833" class="links">Computing and Optimization for the Physical and Social Sciences</a> 
                              , Princeton University, Fall 2023.
                              <br></summary>
                          </details>
                        </td>
                      </tr>
                    </div>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <br>


                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Teaching</heading>
                      </td>
                    </tr>
                  </tbody>
                </table>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                    <tr>
                      <td style="padding-left:25px;width:5%;vertical-align:middle">
                        <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                      </td>
                      <td width="95%" valign="middle" style="padding-left:20px">
                        <details>
                          <summary><a href="https://orfe.princeton.edu/courses/spring-2023/optimization" class="links">Special Topics in Recent Developments in Deep Learning and Large Language Models</a> 
                            , Texas A&M University, Fall 2024.
                            <br></summary>
                        </details>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
                        <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                      </td>
                      <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                        <details>
                          <summary><a href="https://registrar.princeton.edu/course-offerings/course-details?term=1242&courseid=012833" class="links">Computing and Optimization for the Physical and Social Sciences</a> 
                            , Princeton University, Fall 2023.
                            <br></summary>
                            <br>
                            <strong>Audience:</strong> Students from various bachelors.
                            <br><br>
                            <strong>Tasks:</strong> Hold office hours, grade homeworks and exams, organization.
                            <br><br>
                            <strong>Contents:</strong> An introduction to several fundamental and practically-relevant areas of modern optimization and numerical computing. 
                            Topics include computational linear algebra, first and second order descent methods, convex sets and functions, basics of linear and semidefinite programming, 
                            optimization for statistical regression and classification, and techniques for dealing with uncertainty and intractability in optimization problems. 
                            Extensive hands-on experience with high-level optimization software. 
                            Applications drawn from operations research, statistics and machine learning, economics, control theory, and engineering.
                            <br><br>
                        </details>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
                        <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                      </td>
                      <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                        <details>
                          <summary><a href="https://registrar.princeton.edu/course-offerings/course-details?term=1224&courseid=012309" class="links">
                            Analysis of Big Data</a>, Princeton University, Spring 2023.
                            <br></summary>
                            <br>
                            <strong>Audience:</strong> Students from various bachelors.
                            <br><br>
                            <strong>Tasks:</strong> Head TA. Teach the precepts, hold office hours, grade homeworks and exams.
                            <br><br>
                            <strong>Contents:</strong> This course is a theoretically oriented introduction to the statistical tools
                            that underpin modern machine learning, whose hallmarks are large datasets and/or complex models. 
                            Topics include a rigorous analysis of dimensionality reduction, 
                            a survey of models ranging from regression to neural networks, 
                            and an analysis of learning algorithms.
                            <br><br>
                        </details>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
                        <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                      </td>
                      <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                        <details>
                          <summary><a href="https://registrar.princeton.edu/course-offerings/course-details?term=1224&courseid=012309" class="links">
                            Analysis of Big Data</a>, Princeton University, Spring 2022.
                            <br></summary>
                            <br>
                            <strong>Audience:</strong> Students from various bachelors.
                            <br><br>
                            <strong>Tasks:</strong> Teach the precepts, hold office hours, grade homeworks and exams.
                            <br><br>
                            <strong>Contents:</strong> This course is a theoretically oriented introduction to the statistical tools
                             that underpin modern machine learning, whose hallmarks are large datasets and/or complex models. 
                             Topics include a rigorous analysis of dimensionality reduction, 
                             a survey of models ranging from regression to neural networks, 
                             and an analysis of learning algorithms.
                            <br><br>
                        </details>
                      </td>
                    </tr>



                        <tr>
                          <td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
                            <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                          </td>
                          <td width="95%" valign="middle" style="padding-left:20px;padding-top:20px">
                            <details>
                              <summary><a href="https://registrar.princeton.edu/course-offerings/course-details?term=1222&courseid=012553" class="links">
                                Energy and Commodities Markets</a>, Princeton University, Fall 2021.
                                <br></summary>
                                <br>
                                <strong>Audience:</strong>  Master in Finance and BSc in Operations Research and Financial Engineering.
                                <br><br>
                                <strong>Tasks:</strong> Teach the precepts, hold office hours, grade homeworks and exams (Python, Excel, and theory).
                                <br><br>
                                <strong>Contents:</strong> This course is an introduction to commodities markets (oil, gas, metals, electricity, etc.), and quantitative approaches to capturing uncertainties in their demand and supply.
                                We start from a financial perspective, and traditional models of commodity spot prices and forward curves. Then we cover modern topics: game theoretic models of energy production (OPEC vs. fracking vs. renewables);
                                quantifying the risk of intermittency of solar and wind output on the reliability of the electric grid (mitigating the duck curve); financialization of commodity markets; carbon emissions markets; cryptocurrencies as commodities.
                                We also discuss economic and policy implications.
                                <br><br>
                            </details>
                            
                          </td>
                        </tr>
      
                        <tr>
                          <td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
                            <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                          </td>
                          <td width="95%" valign="middle" style="padding-top:20px;padding-left:20px">
                            <details>
                              <summary><a href="http://vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?lerneinheitId=136107&semkez=2020S&ansicht=KATALOGDATEN&lang=en" class="links">
                                Numerical Methods for Partial Differential Equations</a>, ETH Zurich, Spring 2020.<br></summary>
                                <br>
                                <strong>Audience:</strong> Master in Physics, Data Science, Computational Biology. Bachelor in Computational Science and Engineering.
                                <br><br>
                                <strong>Tasks:</strong> Teach the precepts and grade the homeworks (C++ and theory).
                                <br><br>
                                <strong>Contents:</strong> Derivation, properties, and implementation of fundamental numerical methods for a few key partial differential equations: 
                                convection-diffusion, heat equation, wave equation, conservation laws. 
                                Implementation in C++ based on a finite element library.
                                <br><br>
                            </details>
                            
                          </td>
                        </tr>

                        <tr>
                          <td style="padding-left:25px;width:5%;vertical-align:middle;padding-top:20px">
                            <i class="fa fa-graduation-cap" aria-hidden="true"></i>
                          </td>
                          <td width="95%" valign="middle" style="padding-top:20px;padding-left:20px">
                            <details>
                              <summary><a href="http://vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?lerneinheitId=132349&semkez=2019W&ansicht=KATALOGDATEN&lang=en" class="links">
                                Computational Methods for Engineering Applications</a>, ETH Zurich, Fall 2019.</summary>
                                <br>
                                <strong>Audience:</strong> Bachelor in Mechanical Engineering.
                                <br><br>
                                <strong>Tasks:</strong> Teach the precepts and grade the homeworks (C++ and theory).
                                <br><br>
                                <strong>Contents:</strong> Introduction to the numerical methods for the solution of ordinary and partial differential equations that play a central role in engineering applications.
                                Both basic theoretical concepts and implementation techniques necessary to understand and master the methods are addressed.
                                <br>
                            </details>
                            
                          </td>
                        </tr>
      
                      </tbody>
                    </table>

                    <br><br>
        </td>
      </tr>
  </table>
</body>

</html>
